% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/DuplicatesManager.R
\name{identify_duplicates}
\alias{identify_duplicates}
\title{Identify Duplicate Files}
\usage{
identify_duplicates(my_dir, glob, algo = "xxhash64", excluded_dir = NULL)
}
\arguments{
\item{my_dir}{the directory to check}

\item{glob}{type of file to search for}

\item{algo}{hashing algorithm to use (see digest::digest)}

\item{excluded_dir}{directories to exclude}
}
\value{
a list
}
\description{
DupeGuru-esque.
No output difference between the different algorithms:
algo = c("md5","crc32", "sha1", "sha256", "sha512",
"xxhash32", "xxhash64", "murmur32", "blake3")
}
\examples{
\dontrun{
my_dir <- fs::path_home("OneDrive/PhD Psychology")

xclude <- c("/00 - Admin/", "/01 - R Project/",
"/PDFs_BackUps/", "/62 Included Psych - Lit Review/")

identify_duplicates(my_dir, glob = "*.pdf", excluded_dir = xclude) \%>\%
janitor::remove_constant()
}

}
